###bs4
1. 获取标签之间的文本
- soup.a.text/string/get_text()
- string获取直系标签下文本
- text,get_text()该标签下所有文本
2. 获取标签的属性值
- soup.a['href']
###xpath

###代理ip
- 快代理
- 西祠代理
- www.goubanjia.com

### scrapy
- 创建项目:scrapy startproject name
- 使用crawlSpider: scrapy genspider -t crawl example example.com
- 运行scrapy crawl name

### 文件说明
- BaiKe：测试文件，测试如何使用requests获得百度百科某个词条的网页内容，可以不看
- neo4j：测试文件，测试测试neo4j数据库的使用
- practice：测试文件，解析百度百科中网页内容，获得需要的信息
- scrapy
  - baike：使用的scrapy进行爬取，后来用crawlSpider替代了
  - crawlSpider：主要功能文件
    - crawlSpider：运行文件，主程序是baike.py，网页解析和处理部分在pipelines中实现。这里获取到的网页信息使用了neo4j进行存储，可以换成mysql进行存储，因为在后端程序中，我也是将neo4j中的数据转储到了mysql中，因为neo4j操作起来很麻烦，而且相应速度比较慢，后来就放弃了，不过neo4j有可视化界面
    - data：存储爬取到的数据，并作为训练数据进行训练
    - train_model.py：使用gensim中的word2vec模型进行训练，训练结果保存在model/word2vec_news.model中



- server：前端程序中会用到几个api，主要是利用jieba分词工具对传来的字符串进行分词、关键词提取等处理
